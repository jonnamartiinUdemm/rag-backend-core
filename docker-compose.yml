version: '3.8'

services:
  # API service
  web:
    build: .
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    ports:
      - "8000:8000"
    volumes:
      - .:/app
      - ./data:/app/data
    depends_on:
      - redis
      - ollama # Web now waits for Ollama to start

  # WORKER service celery
  worker:
    build: .
    command: celery -A app.core.celery_app worker --loglevel=info
    volumes:
      - .:/app
      - ./data:/app/data
    depends_on:
      - redis

  # Redis service
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"

  # Qdrant service (Vector DB)
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage

  # --- NEW SERVICE: OLLAMA (Local AI Brain) ---
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434" # Standard Ollama port
    volumes:
      - ollama_data:/root/.ollama

# Persistent volumes
volumes:
  qdrant_data:
  ollama_data: